# STAT306 project

## 1. Introduction

#### Motivation

Nowadays, as technological advancements revolutionize industries, digital car trading platforms have emerged as vital marketplaces, transforming the automotive market and the way people buy and sell second-hand vehicles. Notably, CarDekho.com is a well-known car trading platform based in India. The primary motivation for this analysis stems from the growing importance of platforms like CarDekho.com in connecting buyers and sellers. In particular, for buyers, understanding the relationship between car traits and pricing can facilitate purchasing decisions, and help assess whether a car is priced fairly. Meanwhile, for sellers, the insights into pricing trends can help attract more potential customers while still setting competitive selling prices, determining the optimal price for their listings. Thus, this study aims to investigate a deeper understanding of these relationships by addressing the following research questions:

-   What are the relationships between the selling price and continuous covariates (year, km_driven)?

-   What categorical variables are associated with the selling price, and how do these variables influence pricing?

-   What regression model can effectively explain and interpret the relationships in the data?

By answering these questions, we aim to develop a statistical model that contributes to the car trading marked by offering data-driven insights.

#### Dataset

The dataset analyzed in this report contains information about second-hand cars provided by CarDekho.com (<https://www.cardekho.com/>). The dataset was published on Kaggle (<https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho/data>) on June 26, 2020.

The following is a description of the variables found in the dataset:

| Variables     | Type        | Descriptions                                                            |
|-----------------|-----------------|--------------------------------------|
| name          | categorical | the name of the car; start with brand name which is a possible factor   |
| year          | continuous  | the year of the car when it was bought (in years)                       |
| selling_price | continuous  | the price of car when it was sold (in Indian Rupee (INR))               |
| km_driven     | continuous  | the distance that the car is driven (in kilometers)                     |
| fuel          | categorical | the fuel type of the car; 5 levels (CNG/Diesel/Electric/LPG/Petrol)     |
| seller_type   | categorical | the type of the seller; 3 levels (Dealer, Individual, Trustmark Dealer) |
| transmission  | categorical | the type of gear transmission; 2 levels (Manual, Automatic)             |
| owner         | categorical | the number of previous owners; 5 levels                                 |

By setting selling_price as our response, we have a total of 5 categorical variables and 2 continuous variables.

## 2. Exploratory Analysis

First we'll run the necessary libraries for the visualizations

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(readr)
library(corrplot)
```

### Import Dataset

This dataset is about the information of used cars from CarDekho (<https://www.cardekho.com/>) collected by its creators. The dataset is provided on Kaggle \[above link\] on June 26,2020.

```{r message=FALSE, warning=FALSE}
car_df <- read_csv("CAR_DEKHO.csv")
head(car_df)
```

### Summary Statistics

```{r message=FALSE, warning=FALSE}
str(car_df) 
```

-   We count with 5 character and 3 numerical variables

```{r}
summary(car_df)
```

-   Most of our variables are categorical

-   For our continuous variables there seem to be a large amount of variance (except for year) which could possibly suggest the presence of outliers.

-   Have values for 1992 to 2024

```{r}
lapply(car_df[c("fuel", "transmission", "owner")], unique)
```

-   5 categories for Fuel, 2 for Transmission and 5 for Owner

### Missing Values

```{r}
colSums(is.na(car_df))  
sum(is.na(car_df))    
```

-   No missing values so that means we are not going to need to perform transformations

### Data preprocessing

```{r}
car_df_brand <- separate(car_df, col = name, sep = " ",into = "brand")
head(car_df_brand)
```

-   split the name column into a brand column, which is a possible factor.

### Visualizations

```{r}
# Plot for 'year'
ggplot(car_df, aes(x = year)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "white") +
  labs(title = "Distribution of Year", x = "Year", y = "Count") +
  theme_minimal()

# Plot for 'selling_price'
ggplot(car_df, aes(x = selling_price)) +
  geom_histogram(binwidth = 50000, fill = "green", color = "white") +
  labs(title = "Distribution of Selling Price", x = "Selling Price (in currency units)", y = "Count") +
  theme_minimal()

# Plot for 'km_driven'
ggplot(car_df, aes(x = km_driven)) +
  geom_histogram(binwidth = 10000, fill = "purple", color = "white") +
  labs(title = "Distribution of KM Driven", x = "Kilometers Driven", y = "Count") +
  theme_minimal()
```

-   We don't seem to count with normal distributions for neither of the 3 variables

-   Year seems to have a left skew distribution

-   Selling Price and KM Driven show a right-skewed distribution, probably hinting on outliers for high values

```{r}
car_df$owner <- factor(car_df$owner, 
                       levels = c("First Owner", 
                                  "Second Owner", 
                                  "Third Owner", 
                                  "Fourth & Above Owner", 
                                  "Test Drive Car"))

# Bar plot with the specified order
ggplot(car_df, aes(x = owner)) +
  geom_bar(fill = "steelblue", color = "black") +
  labs(title = "Distribution of Owner Variable",
       x = "Owner",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10),  # Smaller and angled text
        plot.title = element_text(size = 14))
```

```{r}
summary(factor(car_df$owner))
```

-   The distribution for the owner variable shows a intuitive pattern where most of the cars offered have just one owner, and then the proportion decreases by almost half for each new owner that has had the vehicle.

```{r}
# Box plot for selling price by owner
ggplot(car_df, aes(x = owner, y = selling_price, fill = owner)) +
  geom_boxplot(outlier.shape = 16, outlier.size = 2) +
  labs(title = "Box Plot of Selling Price by owners", 
       x = "Owner", 
       y = "Selling Price (in currency units)") +
  theme_minimal() +
  theme(legend.position = "none") 
```

-   Outliers seem to be making hard the interpretation for difference in selling price depending on the type of owners

-   we still observe that cars with less previous owners have higher selling prices.

```{r}
# Box plot for selling price by brand
ggplot(car_df_brand, aes(x = brand, y = selling_price, fill = brand)) +
  geom_boxplot(outlier.shape = 16, outlier.size = 2) +
  labs(title = "Box Plot of Selling Price by brands", 
       x = "brand", 
       y = "Selling Price (in currency units)") +
  theme_minimal() +
  theme(legend.position = "none")
```

-   There is difference in selling prices among different brands, specifically, Audi, BMW, Land Rover, Benz, Volvo has higher selling prices.

-   However, the number of cars are greatly different among these brands. For model construction, we may merge brands into two levels : \[luxury, economical\] by the median selling price of each brand. According to the plot, we use 1250000 as the threshold

```{r}
car_df_brand |> 
  select(brand, selling_price) |>
  group_by(brand) |>
  summarise(median_price = median(selling_price)) |>
  filter(median_price > 1250000)
```

The above brands is considered "luxury"; the rest is considered "economical"

```{r}
# New dataset with brand_level
car_df2 <- car_df_brand %>% 
  mutate(brand_level = if_else(brand  %in% c("Audi", "BMW", "Isuzu",
                             "Jaguar", "Jeep", "Kia",
                             "Land", "MG", "Mercedes-Benz",
                             "Volvo"),
                             "luxury", "economical"))
```

```{r}
# Box plot for selling price by brand level
ggplot(car_df2, aes(x = brand_level, y = selling_price, fill = brand_level)) +
  geom_boxplot(outlier.shape = 16, outlier.size = 2) +
  labs(title = "Box Plot of Selling Price by brand level", 
       x = "brand level", 
       y = "Selling Price (in currency units)") +
  theme_minimal() +
  theme(legend.position = "none") 
```

```{r}
# Box plot for selling price by fuel type
ggplot(car_df, aes(x = fuel, y = selling_price, fill = fuel)) +
  geom_boxplot(outlier.shape = 16, outlier.size = 2) +
  labs(title = "Box Plot of Selling Price by Fuel Type", 
       x = "Fuel Type", 
       y = "Selling Price (in currency units)") +
  theme_minimal() +
  theme(legend.position = "none") 
```

-   Outliers seem to be making hard the interpretation for difference in selling price depending on the type of Fuel

-   Diesel and Petrol seem to have most of the outliers but could be because of the size of the data for each type

```{r message=FALSE, warning=FALSE}

lower_bound <- quantile(car_df$selling_price, 0.05, na.rm = TRUE)  
upper_bound <- quantile(car_df$selling_price, 0.95, na.rm = TRUE)  

# Box plot for selling price by fuel type (without outliers)
ggplot(car_df, aes(x = fuel, y = selling_price, fill = fuel)) +
  geom_boxplot(outlier.shape = NA) +
  labs(title = "Box Plot of Selling Price by Fuel Type (Without Outliers)", 
       x = "Fuel Type", 
       y = "Selling Price (in currency units)") +
  scale_y_continuous(limits = c(lower_bound, upper_bound)) +
  theme_minimal() +
  theme(legend.position = "none")
```

-   Without outliers we can see better that there could potentially be a difference in the mean selling price depending on the fuel type

-   Also, we can better understand the distribution of the selling price for each fuel type

```{r}
ggplot(car_df, aes(x = year, y = selling_price)) +
  geom_point(alpha = 0.6, color = "blue") +
  labs(title = "Scatter Plot of Selling Price vs. Year",
       x = "Year",
       y = "Selling Price (in currency units)") +
  theme_minimal()
```

-   For year and selling price, even though we don't have a straight away linear regression, we can see that the overall values seem to increase as the year go buy, which is intuitive since older cars always tend to have lower prices.

```{r}
ggplot(car_df, aes(x = year, y = selling_price)) +
  geom_point(aes(color = transmission), alpha = 0.6) +
  labs(title = "Scatter Plot of Selling Price vs. Year by Transmission Type",
       x = "Year",
       y = "Selling Price (in currency units)",
       color = "Transmission") +
  facet_wrap(~ transmission) +  # Create separate plots for each transmission type
  theme_minimal() +
  theme(legend.position = "none")
```

-   When we add the transmission variable to this plot, we can see that the analysis changes, since now we can see a more linear relationship for Manual cars, when as for Automatic there seems to be more variance and a potential higher slope

```{r}
num_vars <- car_df[sapply(car_df, is.numeric)]

cor_matrix <- cor(num_vars)

corrplot(cor_matrix, method = 'color', order = 'alphabet', tl.col = "black", tl.srt = 45, addCoef.col = "black")
```

-   The correlation matrix supports our hypothesis that newer cars (higher year value) tend to be more expensive. This is reflected in a positive correlation with selling price, though it is not as strong as expected.

-   Kilometers driven have a negative correlation with selling price, meaning fewer kilometers result in higher prices. This aligns with the logic that a car with low mileage is closer to being new and therefore more valuable.

-   The correlation between year and km_driven shows that newer cars are less likely to have high mileage, as older cars have been in use longer. This results in a significant negative correlation.

-   This correlation between covariates may lead to multicollinearity, we need to consider this in statistical analysis.

## 3. Statistical Analysis

### (a) Multicollinearity

First we explore the performance of additive model

We use VIF (variance inflation factor) to check multicollinearity between covariates

```{r}
car_df2[c("fuel", "transmission", "owner", "seller_type", "brand_level")] <- map_dfc(car_df2[c("fuel", "transmission", "owner", "seller_type", "brand_level")],factor)
covariates_df = select(car_df2, -brand, -selling_price)
```

```{r}
year_reg = lm(data = covariates_df, year~.)
VIF_year = 1 / (1-summary(year_reg)$r.squared)
print(paste("VIF(year):", round(VIF_year,3)))

km_driven_reg = lm(data = covariates_df, km_driven~.)
VIF_km_driven = 1 / (1-summary(km_driven_reg)$r.squared)
print(paste("VIF(km_driven):", round(VIF_km_driven,3)))
```

By rule of thumb, VIFs of continuous variables are less than 10, so there is no multicollinearity issues.

### (b) Model with interaction between continuous variables

#### (i) Explore the additive model

We compare the model without interaction of continuous covariates and with interaction

```{r}
#model without continuous interaction, pure additive
mod_add <- lm(data = car_df2, selling_price ~ km_driven + year 
                 + fuel + seller_type + transmission + owner + brand_level )
summary(mod_add)
```

```{r}
#model with continuous interaction
mod_inter <- lm(data = car_df2, selling_price ~ km_driven * year 
                 + fuel + seller_type + transmission + owner + brand_level )
summary(mod_inter)
```

-   Continuous covariates and their interaction are significantly affect the selling price (Research Question 1)

-   For fuel type, only diesel car is significantly different from the baseline (CNG), we may merge the rest of fuel types into "non_diesel"

-   For seller type, only Trustmark dealer is significantly different from the baseline (dealer)

-   there is a significant difference in the coefficient of transmission

-   there is a significant difference in the coefficient of brand level

-   For owners, only the coefficient of "Second Owner" show a significance. May merge the other levels

#### (ii) Diagnostic Diagrams

```{r}
# residuals plot
plot(x=mod_inter$fitted.values, y=mod_inter$residuals)
```

-   Obvious "fan" shape, violating the assumption of constant variance

-   transformation is needed

```{r}
# Q-Q plot
qqnorm(mod_inter$residuals, pch = 1)
qqline(mod_inter$residuals, col = "steelblue", lwd = 2)
```

-   Data points deviate the Q-Q line, suggesting heavy tail distribution of residuals

-   The assumption of normality may be violated

### (c) Log transformation on response

#### (i) fit model with log-transformed response

```{r}
# log transformation on selling_price
log_price = log(car_df2$selling_price)

# fit the same model
log_model =  lm(data = car_df2, log_price ~ km_driven * year 
                 + fuel + seller_type + transmission + owner + brand_level )
summary(log_model)

# residual and QQ plot

plot(x=log_model$fitted.values, y=log_model$residuals)
qqnorm(log_model$residuals, pch = 1)
qqline(log_model$residuals, col = "steelblue", lwd = 2)
```

Comments:

Compared with previous model,

-   More coefficients of owner are significant

-   All coefficients of seller_type are significant

-   Only levels of fuel may be merged

-   R-squared increases from 0.6288 to 0.7252, more variation in selling price is explained

-   In the residuals plot, there is no more obvious patterns

-   In the Q-Q plot, there are still data points deviating the Q-Q line but less extreme. Although there are still concerns about the normality of the error term, we may assume it is approximately normal with slight heavier tails. For better interpretation (Research Question 3), we only use log transformation.

#### (ii) merge the levels in fuel

```{r}
# create the data set with log_selling_price
# merge levels of fuel, "Diesel", "Non-Diesel" to simplify further model selection
car_df2 <- mutate(car_df2, fuel = if_else(fuel == "Diesel","Diesel","Non-diesel"))
car_df2$fuel <- relevel(factor(car_df2$fuel), ref = "Non-diesel")
car <- car_df2[c("year","km_driven","seller_type","transmission","owner","brand_level","fuel")]
car["log_price"] = log_price
# summary(car)
# new data frame, "car"
```

```{r}
log_model_merge =  lm(data = car_df2, log_price ~ km_driven * year 
                 + fuel + seller_type + transmission + owner + brand_level )
summary(log_model_merge)
```

Comment:

-   All coefficients but the coefficient for Test Drive Car are significant

-   it's unnecessary to use model selection procedure to remove covariates (since all significant)

### (d) Interaction Selection

The effective interpretation of model is one of our research goals. So far, the log-transformed model is a better fit than the previous model. Since there are categorical covariates with multiple levels, it is difficult to determine which interactions should be included. For the principle of parsimony, let $\alpha_c = 0.05$, we perform the following backward selection method:

-   To keep the model simple, we only consider the interaction between one categorical and one continuous variable. To be specific, we are only interested in the interaction:

    -   *year:owner* to see whether the expected selling price change differently as the year of car bought change, depending on different number of previous owners.

    -   *km_driven:owner* to see whether the expected selling price change differently as the kilometers driven change, depending on different number of previous owners.

    -   *year:transmission* to see whether the expected selling price change differently as the year of car bought change, depending on different type of transmission.

    -   *km_driven:transmission* to see whether the expected selling price change differently as the kilometers driven change, depending on different type of transmission.

    -   *km_driven:year* The interaction continuous term is also included

-   Start with the full model (with all mentioned four interactions)

-   add one interaction term and fit a new model

-   If the lowest p-value among levels of categorical covariates greater than $\alpha_c (0.05)$, we remove that interaction item

-   iterate the procedure until the end.

#### (i) Selection Procedure

```{r}
full_model <- lm(data = car, car$log_price ~ km_driven * year 
                 + fuel
                 + seller_type
                 + transmission + transmission:km_driven + transmission:year
                 + owner + owner:km_driven + owner:year
                 + brand_level)
# summary(full_model)

model_i2 <- lm(data = car, car$log_price ~ km_driven * year
                 + fuel
                 + seller_type
                 + transmission + transmission:year
                 + owner + owner:km_driven + owner:year
                 + brand_level)
# summary(model_i2)
# 
model_i3 <- lm(data = car, car$log_price ~ km_driven + year
                 + fuel
                 + seller_type
                 + transmission + transmission:year
                 + owner + owner:km_driven + owner:year
                 + brand_level)
# summary(model_i3)

model_i4 <- lm(data = car, car$log_price ~ km_driven + year
                 + fuel
                 + seller_type
                 + transmission
                 + owner + owner:km_driven + owner:year
                 + brand_level)
summary(model_i4)
```

| iteration | remove term              | p-value |
|-----------|--------------------------|---------|
| 1         | *km_driven:transmission* | 0.665   |
| 2         | *km_driven:year*         | 0.436   |
| 3         | *year:transmission*      | 0.149   |

```{r}
###
# Skip this part, it's NOT used anymore!

###
# model1 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:km_driven
#                  + seller_type
#                  + transmission
#                  + owner
#                  + brand_level)
# summary(model1)
# model2 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type
#                  + transmission
#                  + owner
#                  + brand_level)
# summary(model2)
# model3 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type + seller_type:km_driven
#                  + transmission
#                  + owner
#                  + brand_level)
# summary(model3)
# model4 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type + seller_type:year
#                  + transmission
#                  + owner
#                  + brand_level)
# summary(model4)

# model5 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type
#                  + transmission + transmission:km_driven
#                  + owner
#                  + brand_level)
# summary(model5)
# model6 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type
#                  + transmission + transmission:year
#                  + owner
#                  + brand_level)
# summary(model6)
# model7 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type
#                  + transmission
#                  + owner + owner:km_driven
#                  + brand_level)
# summary(model7)

# model8 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type
#                  + transmission
#                  + owner + owner:km_driven + owner:year
#                  + brand_level)
# summary(model8)

# model9 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type
#                  + transmission
#                  + owner + owner:km_driven + owner:year
#                  + brand_level + brand_level:km_driven)
# summary(model9)
# model10 = lm(data = car, car$log_price ~ km_driven * year
#                  + fuel + fuel:year
#                  + seller_type
#                  + transmission
#                  + owner + owner:km_driven + owner:year
#                  + brand_level + brand_level:year)
# summary(model10)
```

Now, we have a model from part(c) with interaction of continuous covariates and a model from part (d) with interactions of categorical covariates. We want to see which model is a better fit.

### (e) Model Comparison

#### (i) Mallow's Cp statistic

calculate the Mallow's Cp statistic for both model

```{r}
# log_model_merge is the model fitted in (c) 
# model_i4 is the model found in (d)
# full_model is defineed in (d)

# add all interaction to create the "full model"
q = length(coef(full_model)) - 1
MSR_q = t(full_model$residuals) %*% full_model$residuals/(nrow(car) - (q+1))
Cp_full = (t(full_model$residuals) %*% full_model$residuals)/MSR_q - (nrow(car)- 2*(q+1))
# c(Cp_full)

# log model in (c) with km_driven:year
p_log = length(coef(log_model_merge)) - 1
SS_log = t(log_model_merge$residuals) %*% log_model_merge$residuals
Cp_log = SS_log/MSR_q - (nrow(car)- 2*(p_log+1))
# (p_log + 1)
# c(Cp_log)

# new found model in (d)
p_4 = length(coef(model_i4)) - 1
SS_4 = t(model_i4$residuals) %*% model_i4$residuals
Cp_4 = SS_4/MSR_q - (nrow(car)- 2*(p_4+1))
# (p_4+1)
# c(Cp_4)

Cp_df = data.frame("Expected(p+1)" = c(p_log + 1, p_4+1), 
                   "Cps" = c(c(Cp_log),c(Cp_4)),
                    row.names = c("log","i4"))
Cp_df
```

-   In terms of Cp, the new found model is better. The C statistic is more close to p+1.

#### (ii) R-squared and adjusted R-squared

```{r}
log_sum <- summary(log_model_merge)
i4_sum <- summary(model_i4)

r_df <- data_frame("R-squared"=c(log_sum$r.squared,i4_sum$r.squared),
                   "Adj R-squared"=c(log_sum$adj.r.squared,i4_sum$adj.r.squared),
                   row.names=c("log","i4"))
r_df
```

-   New founded model is slightly better in explaining the variation in response

#### (iii) Diagnostic diagram for the new found model

```{r}
plot(x=model_i4$fitted.values, y=model_i4$residuals)
qqnorm(model_i4$residuals, pch = 1)
qqline(model_i4$residuals, col = "steelblue", lwd = 2)
```

-   Still, no strong violations

#### (iv) Finalize model

Comment: In terms of R-squared and Cp statistic, the new found model is a better fit. Also, it is easier to explain the interaction between categorical covariate and continuous covariate compared to the interaction between two continuous covariates. Hence, we choose the new found model to discuss.

## 4. Discussion

-   Answer Research Question 1,2,3

-   Describe the new found model

-   we made extra assumption of normal distribution of error term

-   limitation: the outliers are hard to handle, though we use log transformation to reduce the effects

-   etc.
